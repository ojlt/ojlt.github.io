<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Owain&#39;s Blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Owain&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 15 Sep 2025 10:56:30 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>So what is PPO anyway?</title>
      <link>http://localhost:1313/posts/ppo/</link>
      <pubDate>Mon, 15 Sep 2025 10:56:30 +0100</pubDate>
      <guid>http://localhost:1313/posts/ppo/</guid>
      <description>So what is PPO anyway? Proximal Policy Optimisation (PPO) is one of the most widely used algorithms in reinforcement learning. It is also a bit hard to understand. Here is its objective function in its full glory: $$ L^{CLIP}(\theta) = \mathbb{E}_{t}\left[\min\left(r_{t}(\theta)\hat{A}_{t}, \text{clip}(r_{t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{t}\right)\right] $$&#xA;Make sense to you? If you&amp;rsquo;re anything like me when you first read it, probably not. This blog post aims to present a clear explanation of the PPO algorithm, aiming for intuition but also rigour, as the elegance of PPO is best appreciated with some maths.</description>
    </item>
  </channel>
</rss>
